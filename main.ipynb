{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dd8669",
   "metadata": {},
   "source": [
    "# Cell 1: Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b71ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK punkt tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "DATA_DIR = 'samples'\n",
    "\n",
    "print(\"Downloading NLTK punkt tokenizer...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84acab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading from: samples\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\Alzheimer. Using parent folders.\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\COPD. Using parent folders.\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\Gastro-oesophageal Reflux Disease. Using parent folders.\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\Heart Failure. Using parent folders.\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\Hyperlipidemia. Using parent folders.\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\Hypertension. Using parent folders.\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\Tuberculosis. Using parent folders.\n",
      "Warning: Could not reliably determine Disease/PDD from path: samples\\Upper Gastrointestinal Bleeding. Using parent folders.\n",
      "Loaded 511 records using path structure.\n",
      "Data loading finished. Number of records loaded: 511\n",
      "\n",
      "--- Sample Loaded Record Structure ---\n",
      "                                                  id         disease_category  \\\n",
      "0  samples\\Acute Coronary Syndrome\\NSTEMI\\1153590...  Acute Coronary Syndrome   \n",
      "1  samples\\Acute Coronary Syndrome\\NSTEMI\\1185908...  Acute Coronary Syndrome   \n",
      "2  samples\\Acute Coronary Syndrome\\NSTEMI\\1199071...  Acute Coronary Syndrome   \n",
      "3  samples\\Acute Coronary Syndrome\\NSTEMI\\1199283...  Acute Coronary Syndrome   \n",
      "4  samples\\Acute Coronary Syndrome\\NSTEMI\\1205401...  Acute Coronary Syndrome   \n",
      "\n",
      "      pdd                                               text  \n",
      "0  NSTEMI  F presents with history of HTN, hypothyroidism...  \n",
      "1  NSTEMI  Admission Labs\\n===============\\n___ 12:45AM B...  \n",
      "2  NSTEMI  ADMISSION LABS\\n==============\\n___ 05:30AM BL...  \n",
      "3  NSTEMI  Female with PMH of rheumatoid arthritis on pre...  \n",
      "4  NSTEMI  ADMISSION LABS\\n___ 12:42PM BLOOD WBC-10.8 RBC...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_text_from_json(data):\n",
    "    \"\"\"\n",
    "    Heuristic to extract the main text content from the JSON data.\n",
    "    Assumes the most relevant text is the longest string value.\n",
    "    \"\"\"\n",
    "    longest_text = \"\"\n",
    "    if isinstance(data, dict):\n",
    "        for value in data.values():\n",
    "            if isinstance(value, str):\n",
    "                if len(value) > len(longest_text):\n",
    "                    longest_text = value\n",
    "            elif isinstance(value, list): # Handle lists of strings\n",
    "                 list_text = \"\\n\".join(filter(lambda x: isinstance(x, str), value))\n",
    "                 if len(list_text) > len(longest_text):\n",
    "                     longest_text = list_text\n",
    "    elif isinstance(data, str): # If the JSON root is just a string\n",
    "        longest_text = data\n",
    "    elif isinstance(data, list): # If the JSON root is a list\n",
    "        list_text = \"\\n\".join(filter(lambda x: isinstance(x, str), data))\n",
    "        if len(list_text) > len(longest_text):\n",
    "            longest_text = list_text\n",
    "\n",
    "    return longest_text.strip()\n",
    "\n",
    "\n",
    "def load_data_from_structure(data_dir):\n",
    "    \"\"\"\n",
    "    Loads JSON data recursively, extracting metadata (Disease Category, PDD)\n",
    "    from the folder structure and text content using a heuristic.\n",
    "    Expected structure: data_dir / Disease Category / PDD / note.json\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    print(f\"Starting data loading from: {data_dir}\")\n",
    "    if not os.path.isdir(data_dir):\n",
    "        print(f\"Error: Data directory '{data_dir}' not found.\")\n",
    "        return []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        # Check if the current directory seems like a PDD directory (contains .json files)\n",
    "        if any(f.endswith('.json') for f in files):\n",
    "            # Try to extract Disease Category and PDD from the path\n",
    "            try:\n",
    "                path_parts = os.path.normpath(root).split(os.sep)\n",
    "                # Expecting structure like [... , data_dir, disease_cat, pdd_cat]\n",
    "                # Find the index of the base data_dir\n",
    "                base_dir_index = -1\n",
    "                norm_data_dir = os.path.normpath(data_dir)\n",
    "                for i, part in enumerate(path_parts):\n",
    "                    # Check if the path up to this part matches the base data directory\n",
    "                    current_path_check = os.path.join(*path_parts[:i+1])\n",
    "                    if os.path.samefile(current_path_check, norm_data_dir):\n",
    "                         base_dir_index = i\n",
    "                         break\n",
    "\n",
    "                if base_dir_index != -1 and len(path_parts) > base_dir_index + 2:\n",
    "                    disease_category = path_parts[base_dir_index + 1]\n",
    "                    pdd_category = path_parts[base_dir_index + 2]\n",
    "                else:\n",
    "                    # Fallback if structure is unexpected\n",
    "                    print(f\"Warning: Could not reliably determine Disease/PDD from path: {root}. Using parent folders.\")\n",
    "                    pdd_category = path_parts[-1] if len(path_parts) > 0 else \"Unknown PDD\"\n",
    "                    disease_category = path_parts[-2] if len(path_parts) > 1 else \"Unknown Disease\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing path structure for {root}: {e}\")\n",
    "                disease_category = \"Unknown Disease\"\n",
    "                pdd_category = \"Unknown PDD\"\n",
    "\n",
    "\n",
    "            for filename in files:\n",
    "                if filename.endswith('.json'):\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            data = json.load(f)\n",
    "\n",
    "                        # Extract text content using the heuristic\n",
    "                        text_content = extract_text_from_json(data)\n",
    "\n",
    "                        if text_content: # Only add if text content is found\n",
    "                            all_records.append({\n",
    "                                'id': file_path, # Use file path as a unique ID\n",
    "                                'disease_category': disease_category,\n",
    "                                'pdd': pdd_category, # Keep 'pdd' key for consistency downstream\n",
    "                                'text': text_content\n",
    "                            })\n",
    "\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Could not decode JSON from {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error processing file {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(all_records)} records using path structure.\")\n",
    "    return all_records\n",
    "\n",
    "raw_data = load_data_from_structure(DATA_DIR)\n",
    "\n",
    "if raw_data:\n",
    "    df_raw = pd.DataFrame(raw_data)\n",
    "else:\n",
    "    df_raw = pd.DataFrame() \n",
    "\n",
    "print(f\"Data loading finished. Number of records loaded: {len(df_raw)}\")\n",
    "\n",
    "if not df_raw.empty:\n",
    "    print(\"\\n--- Sample Loaded Record Structure ---\")\n",
    "    print(df_raw.head())\n",
    "else:\n",
    "    print(\"\\nNo data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89476e",
   "metadata": {},
   "source": [
    "# Cell 3: Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642a63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First 5 Records ---\n",
      "                                                  id         disease_category  \\\n",
      "0  samples\\Acute Coronary Syndrome\\NSTEMI\\1153590...  Acute Coronary Syndrome   \n",
      "1  samples\\Acute Coronary Syndrome\\NSTEMI\\1185908...  Acute Coronary Syndrome   \n",
      "2  samples\\Acute Coronary Syndrome\\NSTEMI\\1199071...  Acute Coronary Syndrome   \n",
      "3  samples\\Acute Coronary Syndrome\\NSTEMI\\1199283...  Acute Coronary Syndrome   \n",
      "4  samples\\Acute Coronary Syndrome\\NSTEMI\\1205401...  Acute Coronary Syndrome   \n",
      "\n",
      "      pdd                                               text  \n",
      "0  NSTEMI  F presents with history of HTN, hypothyroidism...  \n",
      "1  NSTEMI  Admission Labs\\n===============\\n___ 12:45AM B...  \n",
      "2  NSTEMI  ADMISSION LABS\\n==============\\n___ 05:30AM BL...  \n",
      "3  NSTEMI  Female with PMH of rheumatoid arthritis on pre...  \n",
      "4  NSTEMI  ADMISSION LABS\\n___ 12:42PM BLOOD WBC-10.8 RBC...  \n",
      "\n",
      "--- Sample Record Details ---\n",
      "id                  samples\\Acute Coronary Syndrome\\NSTEMI\\1153590...\n",
      "disease_category                              Acute Coronary Syndrome\n",
      "pdd                                                            NSTEMI\n",
      "text                F presents with history of HTN, hypothyroidism...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "--- Value Counts for PDD (Top 10) ---\n",
      "pdd\n",
      "Heart Failure                             52\n",
      "Gastro-oesophageal Reflux Disease         41\n",
      "Hypertension                              32\n",
      "NSTEMI                                    28\n",
      "Relapsing-Remitting Multiple Sclerosis    22\n",
      "UA                                        22\n",
      "Low-risk PE                               21\n",
      "Gastric Ulcers                            20\n",
      "COPD                                      19\n",
      "Bacterial Pneumonia                       16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Basic Stats ---\n",
      "Total records: 511\n",
      "Unique PDDs: 55\n"
     ]
    }
   ],
   "source": [
    "if not df_raw.empty:\n",
    "    print(\"--- First 5 Records ---\")\n",
    "    print(df_raw.head())\n",
    "    print(\"\\n--- Sample Record Details ---\")\n",
    "    if len(df_raw) > 0:\n",
    "        print(df_raw.iloc[0])\n",
    "    print(\"\\n--- Value Counts for PDD (Top 10) ---\")\n",
    "    print(df_raw['pdd'].value_counts().head(10))\n",
    "    print(f\"\\n--- Basic Stats ---\")\n",
    "    print(f\"Total records: {len(df_raw)}\")\n",
    "    print(f\"Unique PDDs: {df_raw['pdd'].nunique()}\")\n",
    "else:\n",
    "    print(\"No data loaded, skipping exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffcd4ae",
   "metadata": {},
   "source": [
    "# Cell 4: Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Kept 511 documents.\n",
      "\n",
      "--- Sample Preprocessed Record ---\n",
      "ID: samples\\Acute Coronary Syndrome\\NSTEMI\\11535902-DS-14.json\n",
      "PDD: NSTEMI\n",
      "Cleaned Text: f presents with history of htn, hypothyroidism, no priorcardiac hx who presented to ed with chest pain. patient endorses right sided chest pain for the last 2 days which worsened today, at which point...\n",
      "Tokens (first 20): ['f', 'presents', 'with', 'history', 'of', 'htn', ',', 'hypothyroidism', ',', 'no', 'priorcardiac', 'hx', 'who', 'presented', 'to', 'ed', 'with', 'chest', 'pain', '.']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Replace multiple whitespace with single space\n",
    "    return text\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    \"\"\"\n",
    "    Applies cleaning and tokenization to the documents.\n",
    "    Filters out documents with empty cleaned text.\n",
    "    \"\"\"\n",
    "    preprocessed_docs = []\n",
    "    for doc in documents:\n",
    "        cleaned_text = clean_text(doc['text'])\n",
    "        if cleaned_text: # Only keep documents with non-empty text after cleaning\n",
    "            # Tokenize for BM25 - simple whitespace split is often sufficient,\n",
    "            # but nltk.word_tokenize is more robust.\n",
    "            tokens = nltk.word_tokenize(cleaned_text)\n",
    "            preprocessed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'pdd': doc['pdd'],\n",
    "                'original_text': doc['text'], # Keep original for context generation\n",
    "                'cleaned_text': cleaned_text,\n",
    "                'tokens': tokens\n",
    "            })\n",
    "        # else:\n",
    "        #     print(f\"Filtered out document {doc['id']} due to empty content after cleaning.\")\n",
    "    print(f\"Preprocessing complete. Kept {len(preprocessed_docs)} documents.\")\n",
    "    return preprocessed_docs\n",
    "\n",
    "\n",
    "if raw_data:\n",
    "    preprocessed_data = preprocess_documents(raw_data)\n",
    "\n",
    "    if preprocessed_data:\n",
    "        print(\"\\n--- Sample Preprocessed Record ---\")\n",
    "        print(f\"ID: {preprocessed_data[0]['id']}\")\n",
    "        print(f\"PDD: {preprocessed_data[0]['pdd']}\")\n",
    "        print(f\"Cleaned Text: {preprocessed_data[0]['cleaned_text'][:200]}...\")\n",
    "        print(f\"Tokens (first 20): {preprocessed_data[0]['tokens'][:20]}\")\n",
    "    else:\n",
    "        print(\"No documents remained after preprocessing.\")\n",
    "else:\n",
    "    print(\"Skipping preprocessing as no raw data was loaded.\")\n",
    "    preprocessed_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991acb4e",
   "metadata": {},
   "source": [
    "# Cell 6: BM25 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d328ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BM25 Indexing...\n",
      "BM25 Index created in 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if preprocessed_data:\n",
    "    print(\"Starting BM25 Indexing...\")\n",
    "    tokenized_corpus = [doc['tokens'] for doc in preprocessed_data]\n",
    "\n",
    "    start_time = time.time()\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    end_time = time.time()\n",
    "    print(f\"BM25 Index created in {end_time - start_time:.2f} seconds.\")\n",
    "else:\n",
    "    print(\"Skipping BM25 Indexing as there is no preprocessed data.\")\n",
    "    bm25 = None\n",
    "    tokenized_corpus = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ddf02",
   "metadata": {},
   "source": [
    "# Cell 7: Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0810669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Retrieval Function\n",
    "\n",
    "def retrieve_documents(query, bm25_index, preprocessed_docs, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieves the top_k most relevant documents for a given query using BM25.\n",
    "    \"\"\"\n",
    "    if not bm25_index or not preprocessed_docs:\n",
    "        print(\"Error: BM25 index or preprocessed data not available.\")\n",
    "        return []\n",
    "\n",
    "    # 1. Preprocess the query (same steps as documents)\n",
    "    cleaned_query = clean_text(query)\n",
    "    tokenized_query = nltk.word_tokenize(cleaned_query)\n",
    "\n",
    "    # 2. Get BM25 scores for the query against all documents\n",
    "    doc_scores = bm25_index.get_scores(tokenized_query)\n",
    "\n",
    "    # 3. Get the indices of the top-k documents\n",
    "    # Ensure we don't request more documents than available\n",
    "    k = min(top_k, len(preprocessed_docs))\n",
    "    top_n_indices = np.argsort(doc_scores)[::-1][:k] # Get indices sorted by score descending\n",
    "\n",
    "    # 4. Retrieve the corresponding documents\n",
    "    retrieved_docs = [preprocessed_docs[i] for i in top_n_indices if doc_scores[i] > 0] # Only return docs with score > 0\n",
    "\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents for query: '{query}'\")\n",
    "    # Optional: Print scores\n",
    "    # for i in top_n_indices:\n",
    "    #      print(f\"  - Doc Index {i}, Score: {doc_scores[i]:.4f}, ID: {preprocessed_docs[i]['id']}\")\n",
    "\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b97712",
   "metadata": {},
   "source": [
    "# Cell 8: Generator Setup (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875735e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "Loading Tokenizer: google/flan-t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Personal Data\\Fast\\Semester 8\\GenAI\\A5\\Code\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\muham\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: google/flan-t5-base (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "try:\n",
    "    print(f\"Loading Tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Loading Model: {model_name} (this may take a while)...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "    print(\"Model and Tokenizer loaded successfully.\")\n",
    "    llm_ready = True\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    print(\"LLM setup failed. Generation will not be possible.\")\n",
    "    tokenizer = None\n",
    "    model = None\n",
    "    llm_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e350e",
   "metadata": {},
   "source": [
    "# Cell 9: RAG Prompt Template and Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "247a269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt Template ---\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Based *only* on the following context regarding diagnostic procedures for specific diseases, please answer the question. Do not use any prior knowledge. If the context does not contain the answer, state that the information is not available in the provided documents.\n",
    "\n",
    "Context:\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer(query, retrieved_docs, model, tokenizer, device, max_length=10000):\n",
    "    \"\"\"\n",
    "    Generates an answer using the LLM based on the query and retrieved context.\n",
    "    \"\"\"\n",
    "    if not model or not tokenizer:\n",
    "        return \"Error: LLM model or tokenizer not available.\"\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant documents were found to answer the question.\"\n",
    "\n",
    "    # 1. Format the context string\n",
    "    # Using original_text might provide richer context for the LLM\n",
    "    context_str = \"\\n\\n---\\n\\n\".join([doc['original_text'] for doc in retrieved_docs])\n",
    "\n",
    "\n",
    "    # 2. Create the full prompt\n",
    "    prompt = PROMPT_TEMPLATE.format(context_str=context_str, query=query)\n",
    "\n",
    "    # 3. Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=10000, truncation=True).to(device) \n",
    "\n",
    "    # 4. Generate the answer\n",
    "    try:\n",
    "        print(\"Generating answer...\")\n",
    "        with torch.no_grad(): \n",
    "             outputs = model.generate(\n",
    "                 **inputs,\n",
    "                 max_length=max_length, \n",
    "                 min_length=10,       \n",
    "                 num_beams=20,          \n",
    "                 early_stopping=True   \n",
    "             )\n",
    "\n",
    "        # 5. Decode the output\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        return \"Error occurred while generating the answer.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de5dc5",
   "metadata": {},
   "source": [
    "# Cell 10: Example Usage / User Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== RAG System Demo ==============================\n",
      "\n",
      "Processing Query: 'What are the typical diagnostic procedures for Congestive Heart Failure?'\n",
      "Retrieved 3 documents for query: 'What are the typical diagnostic procedures for Congestive Heart Failure?'\n",
      "Generating answer...\n",
      "\n",
      "Query: What are the typical diagnostic procedures for Congestive Heart Failure?\n",
      "Generated Answer:\n",
      "pulmonary edema and small bilateral effusions\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Processing Query: 'Describe the process for diagnosing Pulmonary Embolism.'\n",
      "Retrieved 3 documents for query: 'Describe the process for diagnosing Pulmonary Embolism.'\n",
      "Generating answer...\n",
      "\n",
      "Query: Describe the process for diagnosing Pulmonary Embolism.\n",
      "Generated Answer:\n",
      "CTA Chest: IMPRESSION: 1. No evidence of pulmonary embolism or acute aortic abnormality. 2. Mucous plugging in bilateral airways, most substantial in the left upper lobe. 3. Left lower lobe pulmonary nodule with somewhat spiculated margins, suspicious for malignancy. 4. 8 x 10 mm left lower lobe pulmonary nodule with somewhat spiculated margins\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Processing Query: 'How is Sepsis diagnosed according to these records?'\n",
      "Retrieved 3 documents for query: 'How is Sepsis diagnosed according to these records?'\n",
      "Generating answer...\n",
      "\n",
      "Query: How is Sepsis diagnosed according to these records?\n",
      "Generated Answer:\n",
      "cTropnT-0.01\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Processing Query: 'Outline the diagnostic steps for Acute Myocardial Infarction.'\n",
      "Retrieved 3 documents for query: 'Outline the diagnostic steps for Acute Myocardial Infarction.'\n",
      "Generating answer...\n",
      "\n",
      "Query: Outline the diagnostic steps for Acute Myocardial Infarction.\n",
      "Generated Answer:\n",
      "05:05PM BLOOD WBC-8.8# RBC-3.38* Hgb-11.4* Hct-35.2* MCV-104* MCH-33.7* MCHC-32.5 RDW-13.6\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Processing Query: 'What is the cause of short of breath?'\n",
      "Retrieved 3 documents for query: 'What is the cause of short of breath?'\n",
      "Generating answer...\n",
      "\n",
      "Query: What is the cause of short of breath?\n",
      "Generated Answer:\n",
      "acute pulmonary edema and an elevated Troponin\n",
      "----------------------------------------------------------------------\n",
      "============================== Demo Finished ==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ask_question(query, bm25_index, preprocessed_docs, model, tokenizer, device, top_k=3):\n",
    "    \"\"\"\n",
    "    Orchestrates the full RAG pipeline: retrieve -> generate.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing Query: '{query}'\")\n",
    "\n",
    "    # 1. Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(query, bm25_index, preprocessed_docs, top_k=top_k)\n",
    "\n",
    "    # Handle case where no documents are retrieved\n",
    "    if not retrieved_docs:\n",
    "        print(\"No relevant documents found by retriever.\")\n",
    "        pass \n",
    "\n",
    "    answer = generate_answer(query, retrieved_docs, model, tokenizer, device)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# User Simulation\n",
    "if bm25 and preprocessed_data and llm_ready:\n",
    "    print(\"\\n\" + \"=\"*30 + \" RAG System Demo \" + \"=\"*30)\n",
    "\n",
    "    # --- Define Sample Queries ---\n",
    "    queries = [\n",
    "        \"What are the typical diagnostic procedures for Congestive Heart Failure?\",\n",
    "        \"Describe the process for diagnosing Pulmonary Embolism.\",\n",
    "        \"How is Sepsis diagnosed according to these records?\",\n",
    "        \"Outline the diagnostic steps for Acute Myocardial Infarction.\",\n",
    "        \"What is the cause of short of breath?\"\n",
    "    ]\n",
    "\n",
    "    # --- Run Queries Through the RAG Pipeline ---\n",
    "    for q in queries:\n",
    "        final_answer = ask_question(\n",
    "            query=q,\n",
    "            bm25_index=bm25,\n",
    "            preprocessed_docs=preprocessed_data,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            top_k=3 # Number of documents to retrieve\n",
    "        )\n",
    "        print(f\"\\nQuery: {q}\")\n",
    "        print(f\"Generated Answer:\\n{final_answer}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "    print(\"=\"*30 + \" Demo Finished \" + \"=\"*30 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
